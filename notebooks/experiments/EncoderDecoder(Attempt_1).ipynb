{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uje9hyHswoLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, RepeatVector, TimeDistributed, Concatenate, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "rZSIGmpNHR6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create sequences for time series prediction\n",
        "def create_sequences(data, n_steps_in, n_steps_out):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
        "        X.append(data[i:i + n_steps_in])\n",
        "        y.append(data[i + n_steps_in:i + n_steps_in + n_steps_out])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Add new function to create cyclical time features\n",
        "def add_cyclical_features(df):\n",
        "    \"\"\"\n",
        "    Add cyclical encoding of time features to capture seasonality\n",
        "    \"\"\"\n",
        "    # For month: convert to sine and cosine components\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['Month']/12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['Month']/12)\n",
        "\n",
        "    # For day of month\n",
        "    days_in_month = 30  # Approximation\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['Day']/days_in_month)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['Day']/days_in_month)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 1. Add this new function for circular wind direction encoding\n",
        "def encode_wind_direction(df):\n",
        "    \"\"\"\n",
        "    Convert wind direction from degrees to sine and cosine components\n",
        "    to properly handle its circular nature\n",
        "    \"\"\"\n",
        "    if 'Wind_Direction' in df.columns:\n",
        "        # Convert degrees to radians and then to sine/cosine components\n",
        "        df['wind_dir_sin'] = np.sin(np.radians(df['Wind_Direction']))\n",
        "        df['wind_dir_cos'] = np.cos(np.radians(df['Wind_Direction']))\n",
        "    return df\n",
        "\n",
        "# 2. Modify the preprocess_data function to include wind direction encoding\n",
        "def preprocess_data(df, base_year=2000):\n",
        "    # Original preprocessing\n",
        "    # Identify numerical columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # Define outlier thresholds using IQR method\n",
        "    Q1 = df[numeric_cols].quantile(0.25)\n",
        "    Q3 = df[numeric_cols].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the column's mean value\n",
        "    for col in numeric_cols:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col] = np.where((df[col] < lower_bound[col]) | (df[col] > upper_bound[col]), mean_value, df[col])\n",
        "\n",
        "    # Convert temperatures > 100 from Kelvin to Celsius\n",
        "    for temp_col in ['Avg_Temperature', 'Avg_Feels_Like_Temperature']:\n",
        "        if temp_col in df.columns:\n",
        "            df[temp_col] = df[temp_col].apply(lambda x: x - 273.15 if x > 100 else x)\n",
        "\n",
        "    # Create date feature if date columns exist\n",
        "    date_cols = [\"Year\", \"Month\", \"Day\"]\n",
        "    if all(col in df.columns for col in date_cols):\n",
        "        df['Date'] = pd.to_datetime(\n",
        "            (df['Year'] + base_year).astype(str) + '-' +\n",
        "            df['Month'].astype(str) + '-' +\n",
        "            df['Day'].astype(str),\n",
        "            format='%Y-%m-%d',\n",
        "            errors='coerce'\n",
        "        )\n",
        "\n",
        "        # Add cyclical features to capture seasonality\n",
        "        df = add_cyclical_features(df)\n",
        "\n",
        "    # Add circular encoding for wind direction\n",
        "    df = encode_wind_direction(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 3. Add this function to convert sine/cosine components back to degrees\n",
        "def wind_direction_from_components(sin_val, cos_val):\n",
        "    \"\"\"Convert sine and cosine components back to degrees\"\"\"\n",
        "    degrees = np.degrees(np.arctan2(sin_val, cos_val))\n",
        "    # Ensure result is in the range [0, 360)\n",
        "    return (degrees + 360) % 360\n",
        "\n",
        "# 4. Add a circular error metric function for wind direction evaluation\n",
        "def circular_mae(actual, predicted, period=360):\n",
        "    \"\"\"\n",
        "    Calculate Mean Absolute Error accounting for circular nature of wind direction\n",
        "\n",
        "    Args:\n",
        "        actual: Array of actual wind direction values in degrees\n",
        "        predicted: Array of predicted wind direction values in degrees\n",
        "        period: The period of the circular variable (360 for wind direction)\n",
        "\n",
        "    Returns:\n",
        "        Mean absolute error in degrees, accounting for circularity\n",
        "    \"\"\"\n",
        "    # Convert to radians for calculation\n",
        "    actual_rad = np.radians(actual)\n",
        "    predicted_rad = np.radians(predicted)\n",
        "\n",
        "    # Calculate circular difference\n",
        "    diff = np.abs(np.arctan2(\n",
        "        np.sin(predicted_rad - actual_rad),\n",
        "        np.cos(predicted_rad - actual_rad)\n",
        "    ))\n",
        "\n",
        "    # Convert back to degrees and return mean\n",
        "    return np.degrees(diff).mean()\n",
        "\n",
        "def process_locations(df):\n",
        "    # Use LabelEncoder for the location names\n",
        "    label_encoder = LabelEncoder()\n",
        "    location_encoded = label_encoder.fit_transform(df['kingdom'])\n",
        "    # Convert to a numpy array and reshape for the model\n",
        "    location_encoded = np.array(location_encoded).reshape(-1, 1)\n",
        "    # Get the vocabulary size for the embedding layer\n",
        "    vocab_size = len(label_encoder.classes_)\n",
        "    return location_encoded, vocab_size, label_encoder\n",
        "\n",
        "# Modified normalize_features function to include cyclical features\n",
        "def normalize_features(df):\n",
        "    # Select numerical features (excluding categorical and ID columns)\n",
        "    # Include the new cyclical features\n",
        "    numerical_cols = [col for col in df.columns if col not in\n",
        "                     ['kingdom', 'ID', 'Date', 'Year', 'Month', 'Day']\n",
        "                     and col in df.columns]\n",
        "\n",
        "    # MinMaxScaler instance\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # Fit and transform the numerical data\n",
        "    numerical_data = scaler.fit_transform(df[numerical_cols])\n",
        "    # Return normalized data and the scaler\n",
        "    return numerical_data, scaler, numerical_cols\n",
        "\n",
        "# Function to find seasonally matching data\n",
        "def get_seasonal_matching_data(train_df, current_month, current_day, kingdom, n_steps_in):\n",
        "    \"\"\"\n",
        "    Get data from training set that matches the season of the current prediction\n",
        "    \"\"\"\n",
        "    # Month range for seasonal matching (same month ±1)\n",
        "    month_lower = max(1, current_month - 1)\n",
        "    month_upper = min(12, current_month + 1)\n",
        "\n",
        "    # Filter by kingdom and season\n",
        "    seasonal_data = train_df[(train_df['kingdom'] == kingdom) &\n",
        "                            (train_df['Month'] >= month_lower) &\n",
        "                            (train_df['Month'] <= month_upper)]\n",
        "\n",
        "    # If not enough data, broaden the month range\n",
        "    if len(seasonal_data) < n_steps_in:\n",
        "        month_lower = max(1, current_month - 2)\n",
        "        month_upper = min(12, current_month + 2)\n",
        "        seasonal_data = train_df[(train_df['kingdom'] == kingdom) &\n",
        "                                (train_df['Month'] >= month_lower) &\n",
        "                                (train_df['Month'] <= month_upper)]\n",
        "\n",
        "    # If still not enough data, use all data for this kingdom\n",
        "    if len(seasonal_data) < n_steps_in:\n",
        "        seasonal_data = train_df[train_df['kingdom'] == kingdom]\n",
        "\n",
        "    # If still not enough, use all training data\n",
        "    if len(seasonal_data) < n_steps_in:\n",
        "        seasonal_data = train_df\n",
        "\n",
        "    return seasonal_data\n",
        "\n",
        "def split_data(location_encoded, numerical_data, test_size=0.1, val_size=0.1):\n",
        "    # Get total sample count\n",
        "    total_samples = len(numerical_data)\n",
        "    # Calculate indices for train/val/test splits\n",
        "    test_start_idx = int(total_samples * (1 - test_size))\n",
        "    val_start_idx = int(total_samples * (1 - test_size - val_size))\n",
        "    # Split data chronologically\n",
        "    X_train = numerical_data[:val_start_idx]\n",
        "    X_val = numerical_data[val_start_idx:test_start_idx]\n",
        "    X_test = numerical_data[test_start_idx:]\n",
        "    loc_train = location_encoded[:val_start_idx]\n",
        "    loc_val = location_encoded[val_start_idx:test_start_idx]\n",
        "    loc_test = location_encoded[test_start_idx:]\n",
        "\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Validation samples: {len(X_val)}\")\n",
        "    print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "    return X_train, X_val, X_test, loc_train, loc_val, loc_test\n",
        "\n",
        "def prepare_sequences(X_train, X_val, X_test, loc_train, loc_val, loc_test, n_steps_in, n_steps_out):\n",
        "    # Create sequences for train, validation, and test data\n",
        "    X_train_seq, y_train_seq = create_sequences(X_train, n_steps_in, n_steps_out)\n",
        "    loc_train_seq, _ = create_sequences(loc_train, n_steps_in, n_steps_out)\n",
        "\n",
        "    X_val_seq, y_val_seq = create_sequences(X_val, n_steps_in, n_steps_out)\n",
        "    loc_val_seq, _ = create_sequences(loc_val, n_steps_in, n_steps_out)\n",
        "\n",
        "    X_test_seq, y_test_seq = create_sequences(X_test, n_steps_in, n_steps_out)\n",
        "    loc_test_seq, _ = create_sequences(loc_test, n_steps_in, n_steps_out)\n",
        "\n",
        "    return (X_train_seq, y_train_seq, loc_train_seq,\n",
        "            X_val_seq, y_val_seq, loc_val_seq,\n",
        "            X_test_seq, y_test_seq, loc_test_seq)"
      ],
      "metadata": {
        "id": "Vw7ggIc5HWZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model functions\n",
        "def build_seq2seq_model(n_steps_in, n_steps_out, n_features, vocab_size, embedding_dim=16):\n",
        "    # Define inputs for features and location\n",
        "    feature_input = Input(shape=(n_steps_in, n_features), name='feature_input')\n",
        "    location_input = Input(shape=(n_steps_in, 1), name='location_input')\n",
        "\n",
        "    # Process location data with embedding\n",
        "    loc_reshaped = Reshape((-1,))(location_input)\n",
        "    loc_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(loc_reshaped)\n",
        "    loc_embedded = Reshape((n_steps_in, embedding_dim))(loc_embedding)\n",
        "\n",
        "    # Combine features with location embeddings\n",
        "    combined = Concatenate(axis=2)([feature_input, loc_embedded])\n",
        "\n",
        "    # Encoder-decoder architecture\n",
        "    encoder = LSTM(128, activation='relu')(combined)\n",
        "    repeat = RepeatVector(n_steps_out)(encoder)\n",
        "    decoder = LSTM(128, activation='relu', return_sequences=True)(repeat)\n",
        "    output = TimeDistributed(Dense(n_features))(decoder)\n",
        "\n",
        "    # Create and compile model\n",
        "    model = Model(inputs=[feature_input, location_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, X_train_seq, y_train_seq, loc_train_seq, X_val_seq, y_val_seq, loc_val_seq, epochs=100, batch_size=64):\n",
        "    # Train with early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        [X_train_seq, loc_train_seq], y_train_seq,\n",
        "        validation_data=([X_val_seq, loc_val_seq], y_val_seq),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "4P3XQfVUHKZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Modified evaluate_model function to handle circular metrics for wind direction\n",
        "def evaluate_model(model, X_test_seq, y_test_seq, loc_test_seq, history, numerical_cols, scaler):\n",
        "    # Make predictions\n",
        "    y_pred = model.predict([X_test_seq, loc_test_seq])\n",
        "\n",
        "    # Calculate overall metrics on normalized values\n",
        "    rmse_overall = math.sqrt(mean_squared_error(y_test_seq.reshape(-1), y_pred.reshape(-1)))\n",
        "    mae_overall = mean_absolute_error(y_test_seq.reshape(-1), y_pred.reshape(-1))\n",
        "\n",
        "    # Inverse transform predictions and actual values to original scale\n",
        "    y_test_original = np.zeros_like(y_test_seq)\n",
        "    y_pred_original = np.zeros_like(y_pred)\n",
        "\n",
        "    for i in range(len(y_test_seq)):\n",
        "        y_test_original[i] = scaler.inverse_transform(y_test_seq[i])\n",
        "        y_pred_original[i] = scaler.inverse_transform(y_pred[i])\n",
        "\n",
        "    # Calculate per-feature metrics on original scale\n",
        "    rmse_values = []\n",
        "    mae_values = []\n",
        "\n",
        "    # Check if we need to handle wind direction components\n",
        "    wind_dir_sin_idx = -1\n",
        "    wind_dir_cos_idx = -1\n",
        "\n",
        "    if 'wind_dir_sin' in numerical_cols and 'wind_dir_cos' in numerical_cols:\n",
        "        wind_dir_sin_idx = numerical_cols.index('wind_dir_sin')\n",
        "        wind_dir_cos_idx = numerical_cols.index('wind_dir_cos')\n",
        "\n",
        "    # Special handling for wind direction if we have sine and cosine components\n",
        "    if wind_dir_sin_idx >= 0 and wind_dir_cos_idx >= 0:\n",
        "        # Convert sine and cosine components back to degrees\n",
        "        actual_wind_dir = []\n",
        "        pred_wind_dir = []\n",
        "\n",
        "        for i in range(y_test_original.shape[0]):\n",
        "            for j in range(y_test_original.shape[1]):\n",
        "                # Convert actual values\n",
        "                actual_sin = y_test_original[i, j, wind_dir_sin_idx]\n",
        "                actual_cos = y_test_original[i, j, wind_dir_cos_idx]\n",
        "                actual_deg = wind_direction_from_components(actual_sin, actual_cos)\n",
        "                actual_wind_dir.append(actual_deg)\n",
        "\n",
        "                # Convert predicted values\n",
        "                pred_sin = y_pred_original[i, j, wind_dir_sin_idx]\n",
        "                pred_cos = y_pred_original[i, j, wind_dir_cos_idx]\n",
        "                pred_deg = wind_direction_from_components(pred_sin, pred_cos)\n",
        "                pred_wind_dir.append(pred_deg)\n",
        "\n",
        "        # Calculate circular error metrics\n",
        "        wind_dir_circular_mae = circular_mae(np.array(actual_wind_dir), np.array(pred_wind_dir))\n",
        "        print(f\"Wind Direction Circular MAE: {wind_dir_circular_mae:.4f} degrees\")\n",
        "\n",
        "    # Calculate standard metrics for all features\n",
        "    for i in range(y_test_seq.shape[2]):\n",
        "        # Skip individual sin/cos components if we already calculated circular metrics\n",
        "        if i == wind_dir_sin_idx or i == wind_dir_cos_idx:\n",
        "            continue\n",
        "\n",
        "        rmse = math.sqrt(mean_squared_error(y_test_original[:,:,i].reshape(-1), y_pred_original[:,:,i].reshape(-1)))\n",
        "        mae = mean_absolute_error(y_test_original[:,:,i].reshape(-1), y_pred_original[:,:,i].reshape(-1))\n",
        "        rmse_values.append(rmse)\n",
        "        mae_values.append(mae)\n",
        "        print(f\"Feature {numerical_cols[i]}: RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
        "\n",
        "    print(f\"Overall Normalized Test RMSE: {rmse_overall:.4f}\")\n",
        "    print(f\"Overall Normalized Test MAE: {mae_overall:.4f}\")\n",
        "\n",
        "    # Visualize results with original scale values\n",
        "    visualize_results(history, y_test_original, y_pred_original, rmse_values, mae_values, numerical_cols)\n",
        "\n",
        "    return rmse_overall, mae_overall, y_pred_original, rmse_values, mae_values"
      ],
      "metadata": {
        "id": "1HM28h0FHeEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(history, y_test_original, y_pred_original, rmse_values, mae_values, feature_names):\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot temperature prediction for a sample (in original scale)\n",
        "    plt.subplot(3, 2, 2)\n",
        "    temp_idx = feature_names.index('Avg_Temperature') if 'Avg_Temperature' in feature_names else 0\n",
        "    plt.plot(y_test_original[0, :, temp_idx], label='Actual')\n",
        "    plt.plot(y_pred_original[0, :, temp_idx], label='Predicted')\n",
        "    plt.title('Prediction vs Actual (Temperature °C)')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Temperature (°C)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot feature-wise RMSE\n",
        "    plt.subplot(3, 2, 3)\n",
        "    # Ensure we're only plotting features that have RMSE values calculated\n",
        "    plot_features = feature_names[:len(rmse_values)]\n",
        "    x_pos = np.arange(len(rmse_values))\n",
        "    plt.bar(x_pos, rmse_values)\n",
        "    plt.xticks(x_pos, plot_features, rotation=90)\n",
        "    plt.title('RMSE by Feature (Original Scale)')\n",
        "    plt.ylabel('RMSE')\n",
        "\n",
        "    # Plot feature-wise MAE\n",
        "    plt.subplot(3, 2, 4)\n",
        "    plt.bar(x_pos, mae_values)\n",
        "    plt.xticks(x_pos, plot_features, rotation=90)\n",
        "    plt.title('MAE by Feature (Original Scale)')\n",
        "    plt.ylabel('MAE')\n",
        "\n",
        "    # Plot additional predictions for important features\n",
        "    if len(feature_names) > 1:\n",
        "        # Plot radiation prediction\n",
        "        rad_idx = feature_names.index('Radiation') if 'Radiation' in feature_names else 1\n",
        "        if rad_idx < y_test_original.shape[2]:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            plt.plot(y_test_original[0, :, rad_idx], label='Actual')\n",
        "            plt.plot(y_pred_original[0, :, rad_idx], label='Predicted')\n",
        "            plt.title('Prediction vs Actual (Radiation W/m²)')\n",
        "            plt.xlabel('Time Step')\n",
        "            plt.ylabel('Radiation (W/m²)')\n",
        "            plt.legend()\n",
        "\n",
        "        # Plot wind speed prediction\n",
        "        wind_idx = feature_names.index('Wind_Speed') if 'Wind_Speed' in feature_names else 2\n",
        "        if wind_idx < y_test_original.shape[2]:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            plt.plot(y_test_original[0, :, wind_idx], label='Actual')\n",
        "            plt.plot(y_pred_original[0, :, wind_idx], label='Predicted')\n",
        "            plt.title('Prediction vs Actual (Wind Speed km/h)')\n",
        "            plt.xlabel('Time Step')\n",
        "            plt.ylabel('Wind Speed (km/h)')\n",
        "            plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('prediction_results.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2Swxgc-eHk1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified run_pipeline function to include monitoring of prediction drift\n",
        "def run_pipeline(df, n_steps_in=30, n_steps_out=7, epochs=50, batch_size=64):\n",
        "    # Process data\n",
        "    # (Using df that already has cyclical features added in preprocess_data)\n",
        "    location_encoded, vocab_size, label_encoder = process_locations(df)\n",
        "    numerical_data, scaler, numerical_cols = normalize_features(df)\n",
        "\n",
        "    # Print numerical columns to verify cyclical features are included\n",
        "    print(f\"Features used in model: {numerical_cols}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, X_test, loc_train, loc_val, loc_test = split_data(location_encoded, numerical_data)\n",
        "\n",
        "    # Prepare sequences\n",
        "    seq_data = prepare_sequences(\n",
        "        X_train, X_val, X_test, loc_train, loc_val, loc_test, n_steps_in, n_steps_out\n",
        "    )\n",
        "    X_train_seq, y_train_seq, loc_train_seq, X_val_seq, y_val_seq, loc_val_seq, X_test_seq, y_test_seq, loc_test_seq = seq_data\n",
        "\n",
        "    # Build model\n",
        "    model = build_seq2seq_model(\n",
        "        n_steps_in=n_steps_in,\n",
        "        n_steps_out=n_steps_out,\n",
        "        n_features=X_train.shape[1],\n",
        "        vocab_size=vocab_size\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # Train model\n",
        "    history = train_model(\n",
        "        model,\n",
        "        X_train_seq, y_train_seq, loc_train_seq,\n",
        "        X_val_seq, y_val_seq, loc_val_seq,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Evaluate model with original scale values\n",
        "    metrics = evaluate_model(\n",
        "        model,\n",
        "        X_test_seq, y_test_seq, loc_test_seq,\n",
        "        history,\n",
        "        numerical_cols,\n",
        "        scaler\n",
        "    )\n",
        "    rmse, mae, y_pred, rmse_values, mae_values = metrics\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        'model': model,\n",
        "        'label_encoder': label_encoder,\n",
        "        'scaler': scaler,\n",
        "        'numerical_cols': numerical_cols,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'rmse_by_feature': rmse_values,\n",
        "        'mae_by_feature': mae_values,\n",
        "        'history': history,\n",
        "        'y_pred': y_pred,\n",
        "        'y_test': y_test_seq\n",
        "    }"
      ],
      "metadata": {
        "id": "wormM2OBHpMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_test(test_df, train_df, n_steps_in=30, n_steps_out=7, target_cols=None, reset_window_days=7):\n",
        "    \"\"\"\n",
        "    Make predictions on test.csv using the trained model with sliding window approach\n",
        "    and seasonal pattern incorporation\n",
        "\n",
        "    Args:\n",
        "        test_df: DataFrame containing test data\n",
        "        train_df: DataFrame containing training data\n",
        "        n_steps_in: Number of input time steps\n",
        "        n_steps_out: Number of output time steps\n",
        "        target_cols: List of target columns to predict\n",
        "        reset_window_days: How often to reset the prediction window with seasonal training data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with predictions for all test rows\n",
        "    \"\"\"\n",
        "    # Default target columns if none provided\n",
        "    if target_cols is None:\n",
        "        target_cols = ['Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction']\n",
        "\n",
        "    # Print information about the data\n",
        "    print(f\"Total rows in test data: {len(test_df)}\")\n",
        "    print(f\"Number of unique kingdoms: {test_df['kingdom'].nunique()}\")\n",
        "\n",
        "    # Preprocess training data to include cyclical features and wind direction encoding\n",
        "    train_df = preprocess_data(train_df)\n",
        "\n",
        "    # Keep track of whether Wind_Direction is among target columns\n",
        "    needs_wind_direction = 'Wind_Direction' in target_cols\n",
        "\n",
        "    # Train the model with enhanced features\n",
        "    results = run_pipeline(train_df, n_steps_in=n_steps_in, n_steps_out=n_steps_out, epochs=50)\n",
        "    model = results['model']\n",
        "    label_encoder = results['label_encoder']\n",
        "    scaler = results['scaler']\n",
        "    numerical_cols = results['numerical_cols']\n",
        "\n",
        "    # Find indices of wind direction components in numerical cols\n",
        "    wind_dir_sin_idx = -1\n",
        "    wind_dir_cos_idx = -1\n",
        "    if 'wind_dir_sin' in numerical_cols and 'wind_dir_cos' in numerical_cols:\n",
        "        wind_dir_sin_idx = numerical_cols.index('wind_dir_sin')\n",
        "        wind_dir_cos_idx = numerical_cols.index('wind_dir_cos')\n",
        "\n",
        "    # Preprocess test data including cyclical features and wind direction encoding\n",
        "    test_df = preprocess_data(test_df)\n",
        "\n",
        "    # Initialize results DataFrame with all test data\n",
        "    result_df = test_df[['ID']].copy()\n",
        "\n",
        "    # Make sure target columns exist in result_df\n",
        "    for col in target_cols:\n",
        "        if col not in result_df.columns:\n",
        "            result_df[col] = np.nan\n",
        "\n",
        "    # Track prediction coverage\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Group by kingdom to handle each location separately\n",
        "    # Sort first by kingdom, then by date to ensure chronological order\n",
        "    test_df = test_df.sort_values(by=['kingdom', 'Year', 'Month', 'Day'])\n",
        "    grouped = test_df.groupby('kingdom')\n",
        "\n",
        "    for kingdom, group in grouped:\n",
        "        print(f\"Processing kingdom: {kingdom}, rows: {len(group)}\")\n",
        "\n",
        "        # Get training data for this kingdom\n",
        "        train_kingdom = train_df[train_df['kingdom'] == kingdom].sort_values(by=['Year', 'Month', 'Day'])\n",
        "\n",
        "        # If kingdom not in training data, use average from all training data\n",
        "        if len(train_kingdom) == 0:\n",
        "            print(f\"Warning: No training data for {kingdom}. Using global averages.\")\n",
        "            train_kingdom = train_df.copy()\n",
        "\n",
        "        # Encode location\n",
        "        try:\n",
        "            location_encoded = label_encoder.transform([kingdom])[0]\n",
        "        except ValueError:\n",
        "            print(f\"Warning: {kingdom} not seen during training. Using a default kingdom.\")\n",
        "            # Use the first kingdom in the training data\n",
        "            default_kingdom = label_encoder.classes_[0]\n",
        "            location_encoded = label_encoder.transform([default_kingdom])[0]\n",
        "\n",
        "        # Track days since last window reset\n",
        "        days_since_reset = reset_window_days  # Start with a reset\n",
        "\n",
        "        # Create initial historical data from seasonally appropriate training data\n",
        "        first_row = group.iloc[0]\n",
        "        current_month = first_row['Month']\n",
        "        current_day = first_row['Day']\n",
        "\n",
        "        # Get seasonal data for initial window\n",
        "        seasonal_data = get_seasonal_matching_data(\n",
        "            train_df, current_month, current_day, kingdom, n_steps_in\n",
        "        )\n",
        "\n",
        "        if len(seasonal_data) >= n_steps_in:\n",
        "            # Use the most recent n_steps_in days from seasonal data\n",
        "            initial_history = seasonal_data.sort_values(\n",
        "                by=['Year', 'Month', 'Day']\n",
        "            ).tail(n_steps_in)[numerical_cols]\n",
        "        else:\n",
        "            # Use what we have and pad with means\n",
        "            initial_history = seasonal_data[numerical_cols]\n",
        "            # Calculate mean values for each feature\n",
        "            mean_values = train_df[numerical_cols].mean()\n",
        "            # Create padding with mean values\n",
        "            padding_rows = n_steps_in - len(initial_history)\n",
        "            padding = pd.DataFrame([mean_values] * padding_rows)\n",
        "            initial_history = pd.concat([padding, initial_history], ignore_index=True)\n",
        "\n",
        "        # Convert to numpy array\n",
        "        history = initial_history.values\n",
        "\n",
        "        # Store previous predictions for RMSE calculation and monitoring\n",
        "        previous_predictions = []\n",
        "        actual_values = []\n",
        "\n",
        "        # For each day in the test set for this kingdom, predict the values\n",
        "        for i in range(len(group)):\n",
        "            current_row = group.iloc[i]\n",
        "            current_id = current_row['ID']\n",
        "            current_month = current_row['Month']\n",
        "            current_day = current_row['Day']\n",
        "\n",
        "            # Check if we need to reset the window with fresh seasonal data\n",
        "            if days_since_reset >= reset_window_days:\n",
        "                print(f\"Resetting prediction window for {kingdom} at month {current_month}, day {current_day}\")\n",
        "\n",
        "                # Get seasonal matching data for current time\n",
        "                seasonal_data = get_seasonal_matching_data(\n",
        "                    train_df, current_month, current_day, kingdom, n_steps_in\n",
        "                )\n",
        "\n",
        "                if len(seasonal_data) >= n_steps_in:\n",
        "                    # Reset history with seasonal data\n",
        "                    seasonal_history = seasonal_data.sort_values(\n",
        "                        by=['Year', 'Month', 'Day']\n",
        "                    ).tail(n_steps_in)[numerical_cols].values\n",
        "\n",
        "                    # Blend current history with seasonal history (gradual transition)\n",
        "                    blend_factor = 0.7  # 70% seasonal, 30% current trajectory\n",
        "                    history = blend_factor * seasonal_history + (1 - blend_factor) * history\n",
        "\n",
        "                    # Reset counter\n",
        "                    days_since_reset = 0\n",
        "\n",
        "            # Create location input (same location for all time steps)\n",
        "            location_seq = np.full((1, n_steps_in, 1), location_encoded)\n",
        "\n",
        "            # Normalize features\n",
        "            history_df = pd.DataFrame(history, columns=numerical_cols)\n",
        "            feature_input_normalized = scaler.transform(history_df).reshape(1, n_steps_in, -1)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = model.predict([feature_input_normalized, location_seq], verbose=0)\n",
        "\n",
        "            # We're only interested in the first prediction (next day)\n",
        "            next_day_pred = prediction[0, 0, :]\n",
        "\n",
        "            # Convert back to original scale\n",
        "            next_day_pred_df = pd.DataFrame(next_day_pred.reshape(1, -1), columns=numerical_cols)\n",
        "            next_day_pred_original = scaler.inverse_transform(next_day_pred_df)\n",
        "\n",
        "            # Store prediction for monitoring\n",
        "            previous_predictions.append(next_day_pred_original[0])\n",
        "\n",
        "            # Handle Wind_Direction prediction from sine and cosine components if needed\n",
        "            if needs_wind_direction and wind_dir_sin_idx >= 0 and wind_dir_cos_idx >= 0:\n",
        "                # Extract sine and cosine components\n",
        "                wind_dir_sin = next_day_pred_original[0, wind_dir_sin_idx]\n",
        "                wind_dir_cos = next_day_pred_original[0, wind_dir_cos_idx]\n",
        "\n",
        "                # Convert back to degrees\n",
        "                wind_dir_degrees = wind_direction_from_components(wind_dir_sin, wind_dir_cos)\n",
        "\n",
        "                # Add to result DataFrame\n",
        "                result_df.loc[result_df['ID'] == current_id, 'Wind_Direction'] = wind_dir_degrees\n",
        "\n",
        "            # Update result DataFrame with predictions for other columns\n",
        "            for j, col in enumerate(numerical_cols):\n",
        "                # Skip wind_dir_sin and wind_dir_cos as we handle them separately above\n",
        "                if col in ['wind_dir_sin', 'wind_dir_cos']:\n",
        "                    continue\n",
        "\n",
        "                if col in target_cols:\n",
        "                    result_df.loc[result_df['ID'] == current_id, col] = next_day_pred_original[0, j]\n",
        "\n",
        "            # Count this prediction\n",
        "            total_predictions += 1\n",
        "\n",
        "            # Prepare for next day prediction by updating history\n",
        "            # Use actual values from test set where available\n",
        "            actual_values_for_next = []\n",
        "            for col_idx, col in enumerate(numerical_cols):\n",
        "                if col in ['wind_dir_sin', 'wind_dir_cos'] and 'Wind_Direction' in current_row and not pd.isna(current_row['Wind_Direction']):\n",
        "                    # Convert actual Wind_Direction to sine and cosine\n",
        "                    actual_wind_dir = current_row['Wind_Direction']\n",
        "                    if col == 'wind_dir_sin':\n",
        "                        actual_values_for_next.append(np.sin(np.radians(actual_wind_dir)))\n",
        "                    else:  # col == 'wind_dir_cos'\n",
        "                        actual_values_for_next.append(np.cos(np.radians(actual_wind_dir)))\n",
        "\n",
        "                    # Store for monitoring\n",
        "                    if col == 'wind_dir_sin' and needs_wind_direction:\n",
        "                        # Calculate the corresponding predicted Wind_Direction value\n",
        "                        wind_dir_sin = next_day_pred_original[0, wind_dir_sin_idx]\n",
        "                        wind_dir_cos = next_day_pred_original[0, wind_dir_cos_idx]\n",
        "                        predicted_wind_dir = wind_direction_from_components(wind_dir_sin, wind_dir_cos)\n",
        "                        actual_values.append(('Wind_Direction', actual_wind_dir, predicted_wind_dir))\n",
        "\n",
        "                elif col in current_row and not pd.isna(current_row[col]):\n",
        "                    # Use actual value from test data\n",
        "                    actual_values_for_next.append(current_row[col])\n",
        "\n",
        "                    # Store for monitoring if this is a target column\n",
        "                    if col in target_cols:\n",
        "                        actual_values.append((col, current_row[col], next_day_pred_original[0, col_idx]))\n",
        "                elif col in target_cols:\n",
        "                    # Use our prediction\n",
        "                    actual_values_for_next.append(next_day_pred_original[0, col_idx])\n",
        "                else:\n",
        "                    # Use the mean value from training\n",
        "                    actual_values_for_next.append(train_df[col].mean())\n",
        "\n",
        "            # Roll the history window forward\n",
        "            history = np.vstack([history[1:], actual_values_for_next])\n",
        "\n",
        "            # Increment days since last reset\n",
        "            days_since_reset += 1\n",
        "\n",
        "        # After processing kingdom, calculate and print RMSE for monitoring\n",
        "        if actual_values:\n",
        "            # Convert to DataFrame for easier analysis\n",
        "            actual_df = pd.DataFrame(actual_values, columns=['feature', 'actual', 'predicted'])\n",
        "\n",
        "            # Calculate RMSE by feature\n",
        "            for feature in actual_df['feature'].unique():\n",
        "                feature_df = actual_df[actual_df['feature'] == feature]\n",
        "                if feature == 'Wind_Direction':\n",
        "                    # Use circular metric for wind direction\n",
        "                    mae = circular_mae(feature_df['actual'], feature_df['predicted'])\n",
        "                    print(f\"Kingdom {kingdom}, Feature {feature}: Circular MAE = {mae:.4f}\")\n",
        "                else:\n",
        "                    # Use standard RMSE for other features\n",
        "                    rmse = np.sqrt(mean_squared_error(feature_df['actual'], feature_df['predicted']))\n",
        "                    print(f\"Kingdom {kingdom}, Feature {feature}: RMSE = {rmse:.4f}\")\n",
        "\n",
        "    # Verify we have predictions for all rows\n",
        "    print(f\"Made predictions for {total_predictions} of {len(test_df)} test rows\")\n",
        "\n",
        "    # Check how many rows have NaN values in target columns\n",
        "    nan_count = result_df[target_cols].isna().any(axis=1).sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"Warning: {nan_count} rows have NaN values in target columns\")\n",
        "\n",
        "    # Keep only ID and target columns in the final output\n",
        "    output_cols = ['ID'] + [col for col in target_cols if col in result_df.columns]\n",
        "    result_df = result_df[output_cols]\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    result_df.to_csv(\"drive/MyDrive/DataCrunch/predictions.csv\", index=False)\n",
        "    print(f\"Predictions saved to predictions.csv with {len(result_df)} rows\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Function to convert sine/cosine components back to degrees\n",
        "def wind_direction_from_components(sin_val, cos_val):\n",
        "    \"\"\"Convert sine and cosine components back to degrees\"\"\"\n",
        "    degrees = np.degrees(np.arctan2(sin_val, cos_val))\n",
        "    # Ensure result is in the range [0, 360)\n",
        "    return (degrees + 360) % 360"
      ],
      "metadata": {
        "id": "cVJ10B1uHtg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH12p-Nm3h8M",
        "outputId": "35547ad9-18db-42fe-8d23-d8cc6172009a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution function\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Load training data\n",
        "    train_df = pd.read_csv(\"drive/MyDrive/DataCrunch/train.csv\")\n",
        "\n",
        "    # Load test data\n",
        "    test_df = pd.read_csv(\"drive/MyDrive/DataCrunch/test.csv\")\n",
        "\n",
        "    # Specify target columns to predict\n",
        "    target_columns = ['Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction']\n",
        "\n",
        "    # Make predictions on test data with sliding window approach\n",
        "    # Reset window every 5 days to prevent drift\n",
        "    predictions = predict_on_test(\n",
        "        test_df=test_df,\n",
        "        train_df=train_df,\n",
        "        n_steps_in=30,\n",
        "        n_steps_out=7,\n",
        "        target_cols=target_columns,\n",
        "        reset_window_days=5  # Reset prediction window every 5 days\n",
        "    )\n",
        "\n",
        "    print(\"Prediction process completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "5wctqdzJHvoy",
        "outputId": "05734117-93f8-4ca9-9c27-f6a51e7b1f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total rows in test data: 4530\n",
            "Number of unique kingdoms: 30\n",
            "Features used in model: ['latitude', 'longitude', 'Avg_Temperature', 'Avg_Feels_Like_Temperature', 'Temperature_Range', 'Feels_Like_Temperature_Range', 'Radiation', 'Rain_Amount', 'Rain_Duration', 'Wind_Speed', 'Wind_Direction', 'Evapotranspiration', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'wind_dir_sin', 'wind_dir_cos']\n",
            "Training samples: 67968\n",
            "Validation samples: 8496\n",
            "Test samples: 8496\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ location_input            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ location_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │            \u001b[38;5;34m480\u001b[0m │ reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ feature_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m18\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m16\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m34\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ feature_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                           │                        │                │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m83,456\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ repeat_vector             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │              \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "│ (\u001b[38;5;33mRepeatVector\u001b[0m)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m131,584\u001b[0m │ repeat_vector[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m18\u001b[0m)          │          \u001b[38;5;34m2,322\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)         │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ location_input            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ location_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">480</span> │ reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ feature_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ feature_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                           │                        │                │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">83,456</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ repeat_vector             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)            │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ time_distributed          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,322</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         │                        │                │                        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m217,842\u001b[0m (850.95 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,842</span> (850.95 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m217,842\u001b[0m (850.95 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">217,842</span> (850.95 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 102ms/step - loss: 0.0420 - val_loss: 0.0201\n",
            "Epoch 2/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 104ms/step - loss: 0.0185 - val_loss: 0.0161\n",
            "Epoch 3/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 105ms/step - loss: 0.0137 - val_loss: 0.0125\n",
            "Epoch 4/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 110ms/step - loss: 0.0108 - val_loss: 0.0113\n",
            "Epoch 5/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 107ms/step - loss: 0.0095 - val_loss: 0.0103\n",
            "Epoch 6/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 108ms/step - loss: 0.0086 - val_loss: 0.0103\n",
            "Epoch 7/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 105ms/step - loss: 0.0082 - val_loss: 0.0095\n",
            "Epoch 8/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 109ms/step - loss: 0.0077 - val_loss: 0.0092\n",
            "Epoch 9/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 101ms/step - loss: 0.0074 - val_loss: 0.0089\n",
            "Epoch 10/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 147ms/step - loss: 0.0072 - val_loss: 0.0090\n",
            "Epoch 11/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 101ms/step - loss: 0.0070 - val_loss: 0.0087\n",
            "Epoch 12/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 99ms/step - loss: 0.0067 - val_loss: 0.0085\n",
            "Epoch 13/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 99ms/step - loss: 0.0066 - val_loss: 0.0085\n",
            "Epoch 14/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 104ms/step - loss: 0.0064 - val_loss: 0.0082\n",
            "Epoch 15/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 106ms/step - loss: 0.0063 - val_loss: 0.0082\n",
            "Epoch 16/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 102ms/step - loss: 0.0061 - val_loss: 0.0081\n",
            "Epoch 17/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 102ms/step - loss: 0.0060 - val_loss: 0.0080\n",
            "Epoch 18/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 100ms/step - loss: 0.0059 - val_loss: 0.0079\n",
            "Epoch 19/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 105ms/step - loss: 0.0058 - val_loss: 0.0078\n",
            "Epoch 20/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 100ms/step - loss: 0.0057 - val_loss: 0.0078\n",
            "Epoch 21/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 100ms/step - loss: 0.0056 - val_loss: 0.0079\n",
            "Epoch 22/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 114ms/step - loss: 0.0056 - val_loss: 0.0078\n",
            "Epoch 23/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 105ms/step - loss: 0.0055 - val_loss: 0.0077\n",
            "Epoch 24/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 104ms/step - loss: 0.0054 - val_loss: 0.0076\n",
            "Epoch 25/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 100ms/step - loss: 0.0053 - val_loss: 0.0076\n",
            "Epoch 26/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 102ms/step - loss: 0.0053 - val_loss: 0.0077\n",
            "Epoch 27/50\n",
            "\u001b[1m1062/1062\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 100ms/step - loss: 0.0052 - val_loss: 0.0077\n",
            "Epoch 28/50\n",
            "\u001b[1m 423/1062\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 97ms/step - loss: 0.0052"
          ]
        }
      ]
    }
  ]
}