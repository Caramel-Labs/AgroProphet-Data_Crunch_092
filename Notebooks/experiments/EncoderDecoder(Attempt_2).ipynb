{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZHxqkt7MYYP",
        "outputId": "7ae2aaf6-0a70-4763-9cf7-9082c0eb0d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "# Install Keras Tuner\n",
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt584yjTF6-C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, RepeatVector, TimeDistributed, Concatenate, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dIXy-TiF7jy"
      },
      "outputs": [],
      "source": [
        "# Function to create sequences for time series prediction\n",
        "def create_sequences(data, n_steps_in, n_steps_out):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
        "        X.append(data[i:i + n_steps_in])\n",
        "        y.append(data[i + n_steps_in:i + n_steps_in + n_steps_out])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Add new function to create cyclical time features\n",
        "def add_cyclical_features(df):\n",
        "    \"\"\"\n",
        "    Add cyclical encoding of time features to capture seasonality\n",
        "    \"\"\"\n",
        "    # For month: convert to sine and cosine components\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['Month']/12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['Month']/12)\n",
        "\n",
        "    # For day of month\n",
        "    days_in_month = 30  # Approximation\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['Day']/days_in_month)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['Day']/days_in_month)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 1. Add this new function for circular wind direction encoding\n",
        "def encode_wind_direction(df):\n",
        "    \"\"\"\n",
        "    Convert wind direction from degrees to sine and cosine components\n",
        "    to properly handle its circular nature\n",
        "    \"\"\"\n",
        "    if 'Wind_Direction' in df.columns:\n",
        "        # Convert degrees to radians and then to sine/cosine components\n",
        "        df['wind_dir_sin'] = np.sin(np.radians(df['Wind_Direction']))\n",
        "        df['wind_dir_cos'] = np.cos(np.radians(df['Wind_Direction']))\n",
        "    return df\n",
        "\n",
        "# 2. Modify the preprocess_data function to include wind direction encoding\n",
        "def preprocess_data(df, base_year=2000):\n",
        "    # Original preprocessing\n",
        "    # Identify numerical columns\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # Define outlier thresholds using IQR method\n",
        "    Q1 = df[numeric_cols].quantile(0.25)\n",
        "    Q3 = df[numeric_cols].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Replace outliers with the column's mean value\n",
        "    for col in numeric_cols:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col] = np.where((df[col] < lower_bound[col]) | (df[col] > upper_bound[col]), mean_value, df[col])\n",
        "\n",
        "    # Convert temperatures > 100 from Kelvin to Celsius\n",
        "    for temp_col in ['Avg_Temperature', 'Avg_Feels_Like_Temperature']:\n",
        "        if temp_col in df.columns:\n",
        "            df[temp_col] = df[temp_col].apply(lambda x: x - 273.15 if x > 100 else x)\n",
        "\n",
        "    # Create date feature if date columns exist\n",
        "    date_cols = [\"Year\", \"Month\", \"Day\"]\n",
        "    if all(col in df.columns for col in date_cols):\n",
        "        df['Date'] = pd.to_datetime(\n",
        "            (df['Year'] + base_year).astype(str) + '-' +\n",
        "            df['Month'].astype(str) + '-' +\n",
        "            df['Day'].astype(str),\n",
        "            format='%Y-%m-%d',\n",
        "            errors='coerce'\n",
        "        )\n",
        "\n",
        "        # Add cyclical features to capture seasonality\n",
        "        df = add_cyclical_features(df)\n",
        "\n",
        "    # Add circular encoding for wind direction\n",
        "    df = encode_wind_direction(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 3. Add this function to convert sine/cosine components back to degrees\n",
        "def wind_direction_from_components(sin_val, cos_val):\n",
        "    \"\"\"Convert sine and cosine components back to degrees\"\"\"\n",
        "    degrees = np.degrees(np.arctan2(sin_val, cos_val))\n",
        "    # Ensure result is in the range [0, 360)\n",
        "    return (degrees + 360) % 360\n",
        "\n",
        "# 4. Add a circular error metric function for wind direction evaluation\n",
        "def circular_mae(actual, predicted, period=360):\n",
        "    \"\"\"\n",
        "    Calculate Mean Absolute Error accounting for circular nature of wind direction\n",
        "\n",
        "    Args:\n",
        "        actual: Array of actual wind direction values in degrees\n",
        "        predicted: Array of predicted wind direction values in degrees\n",
        "        period: The period of the circular variable (360 for wind direction)\n",
        "\n",
        "    Returns:\n",
        "        Mean absolute error in degrees, accounting for circularity\n",
        "    \"\"\"\n",
        "    # Convert to radians for calculation\n",
        "    actual_rad = np.radians(actual)\n",
        "    predicted_rad = np.radians(predicted)\n",
        "\n",
        "    # Calculate circular difference\n",
        "    diff = np.abs(np.arctan2(\n",
        "        np.sin(predicted_rad - actual_rad),\n",
        "        np.cos(predicted_rad - actual_rad)\n",
        "    ))\n",
        "\n",
        "    # Convert back to degrees and return mean\n",
        "    return np.degrees(diff).mean()\n",
        "\n",
        "def process_locations(df):\n",
        "    # Use LabelEncoder for the location names\n",
        "    label_encoder = LabelEncoder()\n",
        "    location_encoded = label_encoder.fit_transform(df['kingdom'])\n",
        "    # Convert to a numpy array and reshape for the model\n",
        "    location_encoded = np.array(location_encoded).reshape(-1, 1)\n",
        "    # Get the vocabulary size for the embedding layer\n",
        "    vocab_size = len(label_encoder.classes_)\n",
        "    return location_encoded, vocab_size, label_encoder\n",
        "\n",
        "# Modified normalize_features function to include cyclical features\n",
        "def normalize_features(df):\n",
        "    # Select numerical features (excluding categorical and ID columns)\n",
        "    # Include the new cyclical features\n",
        "    numerical_cols = [col for col in df.columns if col not in\n",
        "                     ['kingdom', 'ID', 'Date', 'Year', 'Month', 'Day']\n",
        "                     and col in df.columns]\n",
        "\n",
        "    # MinMaxScaler instance\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # Fit and transform the numerical data\n",
        "    numerical_data = scaler.fit_transform(df[numerical_cols])\n",
        "    # Return normalized data and the scaler\n",
        "    return numerical_data, scaler, numerical_cols\n",
        "\n",
        "# Function to find seasonally matching data\n",
        "def get_seasonal_matching_data(train_df, current_month, current_day, kingdom, n_steps_in):\n",
        "    \"\"\"\n",
        "    Get data from training set that matches the season of the current prediction\n",
        "    \"\"\"\n",
        "    # Month range for seasonal matching (same month ±1)\n",
        "    month_lower = max(1, current_month - 1)\n",
        "    month_upper = min(12, current_month + 1)\n",
        "\n",
        "    # Filter by kingdom and season\n",
        "    seasonal_data = train_df[(train_df['kingdom'] == kingdom) &\n",
        "                            (train_df['Month'] >= month_lower) &\n",
        "                            (train_df['Month'] <= month_upper)]\n",
        "\n",
        "    # If not enough data, broaden the month range\n",
        "    if len(seasonal_data) < n_steps_in:\n",
        "        month_lower = max(1, current_month - 2)\n",
        "        month_upper = min(12, current_month + 2)\n",
        "        seasonal_data = train_df[(train_df['kingdom'] == kingdom) &\n",
        "                                (train_df['Month'] >= month_lower) &\n",
        "                                (train_df['Month'] <= month_upper)]\n",
        "\n",
        "    # If still not enough data, use all data for this kingdom\n",
        "    if len(seasonal_data) < n_steps_in:\n",
        "        seasonal_data = train_df[train_df['kingdom'] == kingdom]\n",
        "\n",
        "    # If still not enough, use all training data\n",
        "    if len(seasonal_data) < n_steps_in:\n",
        "        seasonal_data = train_df\n",
        "\n",
        "    return seasonal_data\n",
        "\n",
        "def split_data(location_encoded, numerical_data, test_size=0.1, val_size=0.1):\n",
        "    # Get total sample count\n",
        "    total_samples = len(numerical_data)\n",
        "    # Calculate indices for train/val/test splits\n",
        "    test_start_idx = int(total_samples * (1 - test_size))\n",
        "    val_start_idx = int(total_samples * (1 - test_size - val_size))\n",
        "    # Split data chronologically\n",
        "    X_train = numerical_data[:val_start_idx]\n",
        "    X_val = numerical_data[val_start_idx:test_start_idx]\n",
        "    X_test = numerical_data[test_start_idx:]\n",
        "    loc_train = location_encoded[:val_start_idx]\n",
        "    loc_val = location_encoded[val_start_idx:test_start_idx]\n",
        "    loc_test = location_encoded[test_start_idx:]\n",
        "\n",
        "    print(f\"Training samples: {len(X_train)}\")\n",
        "    print(f\"Validation samples: {len(X_val)}\")\n",
        "    print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "    return X_train, X_val, X_test, loc_train, loc_val, loc_test\n",
        "\n",
        "def prepare_sequences(X_train, X_val, X_test, loc_train, loc_val, loc_test, n_steps_in, n_steps_out):\n",
        "    # Create sequences for train, validation, and test data\n",
        "    X_train_seq, y_train_seq = create_sequences(X_train, n_steps_in, n_steps_out)\n",
        "    loc_train_seq, _ = create_sequences(loc_train, n_steps_in, n_steps_out)\n",
        "\n",
        "    X_val_seq, y_val_seq = create_sequences(X_val, n_steps_in, n_steps_out)\n",
        "    loc_val_seq, _ = create_sequences(loc_val, n_steps_in, n_steps_out)\n",
        "\n",
        "    X_test_seq, y_test_seq = create_sequences(X_test, n_steps_in, n_steps_out)\n",
        "    loc_test_seq, _ = create_sequences(loc_test, n_steps_in, n_steps_out)\n",
        "\n",
        "    return (X_train_seq, y_train_seq, loc_train_seq,\n",
        "            X_val_seq, y_val_seq, loc_val_seq,\n",
        "            X_test_seq, y_test_seq, loc_test_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOP4rlANF_AB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HM28h0FHeEp"
      },
      "outputs": [],
      "source": [
        "# 5. Modified evaluate_model function to handle circular metrics for wind direction\n",
        "def evaluate_model(model, X_test_seq, y_test_seq, loc_test_seq, history, numerical_cols, scaler):\n",
        "    # Make predictions\n",
        "    y_pred = model.predict([X_test_seq, loc_test_seq])\n",
        "\n",
        "    # Calculate overall metrics on normalized values\n",
        "    rmse_overall = math.sqrt(mean_squared_error(y_test_seq.reshape(-1), y_pred.reshape(-1)))\n",
        "    mae_overall = mean_absolute_error(y_test_seq.reshape(-1), y_pred.reshape(-1))\n",
        "\n",
        "    # Inverse transform predictions and actual values to original scale\n",
        "    y_test_original = np.zeros_like(y_test_seq)\n",
        "    y_pred_original = np.zeros_like(y_pred)\n",
        "\n",
        "    for i in range(len(y_test_seq)):\n",
        "        y_test_original[i] = scaler.inverse_transform(y_test_seq[i])\n",
        "        y_pred_original[i] = scaler.inverse_transform(y_pred[i])\n",
        "\n",
        "    # Calculate per-feature metrics on original scale\n",
        "    rmse_values = []\n",
        "    mae_values = []\n",
        "\n",
        "    # Check if we need to handle wind direction components\n",
        "    wind_dir_sin_idx = -1\n",
        "    wind_dir_cos_idx = -1\n",
        "\n",
        "    if 'wind_dir_sin' in numerical_cols and 'wind_dir_cos' in numerical_cols:\n",
        "        wind_dir_sin_idx = numerical_cols.index('wind_dir_sin')\n",
        "        wind_dir_cos_idx = numerical_cols.index('wind_dir_cos')\n",
        "\n",
        "    # Special handling for wind direction if we have sine and cosine components\n",
        "    if wind_dir_sin_idx >= 0 and wind_dir_cos_idx >= 0:\n",
        "        # Convert sine and cosine components back to degrees\n",
        "        actual_wind_dir = []\n",
        "        pred_wind_dir = []\n",
        "\n",
        "        for i in range(y_test_original.shape[0]):\n",
        "            for j in range(y_test_original.shape[1]):\n",
        "                # Convert actual values\n",
        "                actual_sin = y_test_original[i, j, wind_dir_sin_idx]\n",
        "                actual_cos = y_test_original[i, j, wind_dir_cos_idx]\n",
        "                actual_deg = wind_direction_from_components(actual_sin, actual_cos)\n",
        "                actual_wind_dir.append(actual_deg)\n",
        "\n",
        "                # Convert predicted values\n",
        "                pred_sin = y_pred_original[i, j, wind_dir_sin_idx]\n",
        "                pred_cos = y_pred_original[i, j, wind_dir_cos_idx]\n",
        "                pred_deg = wind_direction_from_components(pred_sin, pred_cos)\n",
        "                pred_wind_dir.append(pred_deg)\n",
        "\n",
        "        # Calculate circular error metrics\n",
        "        wind_dir_circular_mae = circular_mae(np.array(actual_wind_dir), np.array(pred_wind_dir))\n",
        "        print(f\"Wind Direction Circular MAE: {wind_dir_circular_mae:.4f} degrees\")\n",
        "\n",
        "    # Calculate standard metrics for all features\n",
        "    for i in range(y_test_seq.shape[2]):\n",
        "        # Skip individual sin/cos components if we already calculated circular metrics\n",
        "        if i == wind_dir_sin_idx or i == wind_dir_cos_idx:\n",
        "            continue\n",
        "\n",
        "        rmse = math.sqrt(mean_squared_error(y_test_original[:,:,i].reshape(-1), y_pred_original[:,:,i].reshape(-1)))\n",
        "        mae = mean_absolute_error(y_test_original[:,:,i].reshape(-1), y_pred_original[:,:,i].reshape(-1))\n",
        "        rmse_values.append(rmse)\n",
        "        mae_values.append(mae)\n",
        "        print(f\"Feature {numerical_cols[i]}: RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
        "\n",
        "    print(f\"Overall Normalized Test RMSE: {rmse_overall:.4f}\")\n",
        "    print(f\"Overall Normalized Test MAE: {mae_overall:.4f}\")\n",
        "\n",
        "    # Visualize results with original scale values\n",
        "    visualize_results(history, y_test_original, y_pred_original, rmse_values, mae_values, numerical_cols)\n",
        "\n",
        "    return rmse_overall, mae_overall, y_pred_original, rmse_values, mae_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzVLVSeqGUSo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P3XQfVUHKZa"
      },
      "outputs": [],
      "source": [
        "def build_seq2seq_model_tunable(hp, n_steps_in, n_steps_out, n_features, vocab_size):\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
        "    embedding_dim = hp.Int('embedding_dim', min_value=8, max_value=32, step=8)\n",
        "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.4, step=0.1)  # Reduce max dropout\n",
        "    activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "\n",
        "    # Define inputs\n",
        "    feature_input = Input(shape=(n_steps_in, n_features), name='feature_input')\n",
        "    location_input = Input(shape=(n_steps_in, 1), name='location_input')\n",
        "\n",
        "    # Embed locations\n",
        "    loc_reshaped = Reshape((-1,))(location_input)\n",
        "    loc_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(loc_reshaped)\n",
        "    loc_embedded = Reshape((n_steps_in, embedding_dim))(loc_embedding)\n",
        "\n",
        "    # Combine features\n",
        "    combined = Concatenate(axis=2)([feature_input, loc_embedded])\n",
        "\n",
        "    # Encoder-Decoder model\n",
        "    encoder = LSTM(lstm_units, activation=activation, dropout=dropout_rate, recurrent_dropout=dropout_rate)(combined)\n",
        "    repeat = RepeatVector(n_steps_out)(encoder)\n",
        "    decoder = LSTM(lstm_units, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)(repeat)\n",
        "    output = TimeDistributed(Dense(n_features))(decoder)\n",
        "\n",
        "    model = Model(inputs=[feature_input, location_input], outputs=output)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Swxgc-eHk1Z"
      },
      "outputs": [],
      "source": [
        "def visualize_results(history, y_test_original, y_pred_original, rmse_values, mae_values, feature_names):\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot temperature prediction for a sample (in original scale)\n",
        "    plt.subplot(3, 2, 2)\n",
        "    temp_idx = feature_names.index('Avg_Temperature') if 'Avg_Temperature' in feature_names else 0\n",
        "    plt.plot(y_test_original[0, :, temp_idx], label='Actual')\n",
        "    plt.plot(y_pred_original[0, :, temp_idx], label='Predicted')\n",
        "    plt.title('Prediction vs Actual (Temperature °C)')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Temperature (°C)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot feature-wise RMSE\n",
        "    plt.subplot(3, 2, 3)\n",
        "    # Ensure we're only plotting features that have RMSE values calculated\n",
        "    plot_features = feature_names[:len(rmse_values)]\n",
        "    x_pos = np.arange(len(rmse_values))\n",
        "    plt.bar(x_pos, rmse_values)\n",
        "    plt.xticks(x_pos, plot_features, rotation=90)\n",
        "    plt.title('RMSE by Feature (Original Scale)')\n",
        "    plt.ylabel('RMSE')\n",
        "\n",
        "    # Plot feature-wise MAE\n",
        "    plt.subplot(3, 2, 4)\n",
        "    plt.bar(x_pos, mae_values)\n",
        "    plt.xticks(x_pos, plot_features, rotation=90)\n",
        "    plt.title('MAE by Feature (Original Scale)')\n",
        "    plt.ylabel('MAE')\n",
        "\n",
        "    # Plot additional predictions for important features\n",
        "    if len(feature_names) > 1:\n",
        "        # Plot radiation prediction\n",
        "        rad_idx = feature_names.index('Radiation') if 'Radiation' in feature_names else 1\n",
        "        if rad_idx < y_test_original.shape[2]:\n",
        "            plt.subplot(3, 2, 5)\n",
        "            plt.plot(y_test_original[0, :, rad_idx], label='Actual')\n",
        "            plt.plot(y_pred_original[0, :, rad_idx], label='Predicted')\n",
        "            plt.title('Prediction vs Actual (Radiation W/m²)')\n",
        "            plt.xlabel('Time Step')\n",
        "            plt.ylabel('Radiation (W/m²)')\n",
        "            plt.legend()\n",
        "\n",
        "        # Plot wind speed prediction\n",
        "        wind_idx = feature_names.index('Wind_Speed') if 'Wind_Speed' in feature_names else 2\n",
        "        if wind_idx < y_test_original.shape[2]:\n",
        "            plt.subplot(3, 2, 6)\n",
        "            plt.plot(y_test_original[0, :, wind_idx], label='Actual')\n",
        "            plt.plot(y_pred_original[0, :, wind_idx], label='Predicted')\n",
        "            plt.title('Prediction vs Actual (Wind Speed km/h)')\n",
        "            plt.xlabel('Time Step')\n",
        "            plt.ylabel('Wind Speed (km/h)')\n",
        "            plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('prediction_results.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGFB1I1qGWd8"
      },
      "outputs": [],
      "source": [
        "# Seq2SeqTuner class with fixed implementation\n",
        "class Seq2SeqTuner(kt.HyperModel):\n",
        "    def __init__(self, n_steps_in, n_steps_out, n_features, vocab_size):\n",
        "        self.n_steps_in = n_steps_in\n",
        "        self.n_steps_out = n_steps_out\n",
        "        self.n_features = n_features\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def build(self, hp):\n",
        "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "        lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=32)\n",
        "        embedding_dim = hp.Int('embedding_dim', min_value=8, max_value=32, step=8)\n",
        "        dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.4, step=0.1)\n",
        "        activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
        "\n",
        "        # Define inputs\n",
        "        feature_input = Input(shape=(self.n_steps_in, self.n_features), name='feature_input')\n",
        "        location_input = Input(shape=(self.n_steps_in, 1), name='location_input')\n",
        "\n",
        "        # Embed locations\n",
        "        loc_reshaped = Reshape((-1,))(location_input)\n",
        "        loc_embedding = Embedding(input_dim=self.vocab_size, output_dim=embedding_dim)(loc_reshaped)\n",
        "        loc_embedded = Reshape((self.n_steps_in, embedding_dim))(loc_embedding)\n",
        "\n",
        "        # Combine features\n",
        "        combined = Concatenate(axis=2)([feature_input, loc_embedded])\n",
        "\n",
        "        # Encoder-Decoder model\n",
        "        encoder = LSTM(lstm_units, activation=activation, dropout=dropout_rate, recurrent_dropout=dropout_rate)(combined)\n",
        "        repeat = RepeatVector(self.n_steps_out)(encoder)\n",
        "        decoder = LSTM(lstm_units, activation=activation, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate)(repeat)\n",
        "        output = TimeDistributed(Dense(self.n_features))(decoder)\n",
        "\n",
        "        model = Model(inputs=[feature_input, location_input], outputs=output)\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, hp, model, *args, **kwargs):\n",
        "        # Define batch size as a hyperparameter\n",
        "        batch_size = hp.Int('batch_size', min_value=16, max_value=128, step=16)\n",
        "        early_stopping_patience = hp.Int('early_stopping_patience', min_value=3, max_value=5)\n",
        "\n",
        "        # Create callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=early_stopping_patience,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=max(1, early_stopping_patience - 2),\n",
        "                min_lr=1e-6\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Make sure we don't duplicate callbacks\n",
        "        if 'callbacks' in kwargs:\n",
        "            del kwargs['callbacks']\n",
        "\n",
        "        # Return history from model.fit\n",
        "        return model.fit(\n",
        "            *args,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            **kwargs\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wormM2OBHpMR"
      },
      "outputs": [],
      "source": [
        "def run_pipeline_with_tuning(df, n_steps_in=30, n_steps_out=7, max_trials=10, execution_per_trial=2):\n",
        "    # Process data\n",
        "    location_encoded, vocab_size, label_encoder = process_locations(df)\n",
        "    numerical_data, scaler, numerical_cols = normalize_features(df)\n",
        "\n",
        "    # Print numerical columns to verify cyclical features are included\n",
        "    print(f\"Features used in model: {numerical_cols}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, X_test, loc_train, loc_val, loc_test = split_data(location_encoded, numerical_data)\n",
        "\n",
        "    # Prepare sequences\n",
        "    seq_data = prepare_sequences(\n",
        "        X_train, X_val, X_test, loc_train, loc_val, loc_test, n_steps_in, n_steps_out\n",
        "    )\n",
        "    X_train_seq, y_train_seq, loc_train_seq, X_val_seq, y_val_seq, loc_val_seq, X_test_seq, y_test_seq, loc_test_seq = seq_data\n",
        "\n",
        "    tuner = kt.BayesianOptimization(\n",
        "        Seq2SeqTuner(\n",
        "            n_steps_in=n_steps_in,\n",
        "            n_steps_out=n_steps_out,\n",
        "            n_features=X_train_seq.shape[2],\n",
        "            vocab_size=vocab_size\n",
        "        ),\n",
        "        objective='val_loss',\n",
        "        max_trials=max_trials,\n",
        "        executions_per_trial=execution_per_trial,\n",
        "        directory='my_weather_tuning',\n",
        "        project_name='seq2seq_weather_forecast'\n",
        "    )\n",
        "\n",
        "    # Start hyperparameter search\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    tuner.search(\n",
        "        [X_train_seq, loc_train_seq], y_train_seq,\n",
        "        validation_data=([X_val_seq, loc_val_seq], y_val_seq),\n",
        "        epochs=50  # Maximum epochs per trial\n",
        "    )\n",
        "\n",
        "    # Get the best hyperparameters\n",
        "    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "    print(\"Best hyperparameters:\")\n",
        "    for param, value in best_hps.values.items():\n",
        "        print(f\"{param}: {value}\")\n",
        "\n",
        "    # Build the model with the best hyperparameters\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "    # Train the final model with the best hyperparameters\n",
        "    batch_size = best_hps.get('batch_size')\n",
        "    early_stopping_patience = best_hps.get('early_stopping_patience')\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=max(1, early_stopping_patience - 2), min_lr=1e-6)\n",
        "    ]\n",
        "\n",
        "    # Train the final model\n",
        "    print(\"Training final model with best hyperparameters...\")\n",
        "    history = model.fit(\n",
        "        [X_train_seq, loc_train_seq], y_train_seq,\n",
        "        validation_data=([X_val_seq, loc_val_seq], y_val_seq),\n",
        "        epochs=100,  # We can train longer as we have early stopping\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate model with original scale values\n",
        "    metrics = evaluate_model(\n",
        "        model,\n",
        "        X_test_seq, y_test_seq, loc_test_seq,\n",
        "        history,\n",
        "        numerical_cols,\n",
        "        scaler\n",
        "    )\n",
        "    rmse, mae, y_pred, rmse_values, mae_values = metrics\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        'model': model,\n",
        "        'label_encoder': label_encoder,\n",
        "        'scaler': scaler,\n",
        "        'numerical_cols': numerical_cols,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae,\n",
        "        'rmse_by_feature': rmse_values,\n",
        "        'mae_by_feature': mae_values,\n",
        "        'history': history,\n",
        "        'y_pred': y_pred,\n",
        "        'y_test': y_test_seq,\n",
        "        'best_hyperparameters': best_hps.values\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfxsUtemGZVb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVJ10B1uHtg6"
      },
      "outputs": [],
      "source": [
        "def predict_on_test(test_df, train_df, n_steps_in=30, n_steps_out=7, target_cols=None, reset_window_days=7):\n",
        "    \"\"\"\n",
        "    Make predictions on test.csv using the trained model with sliding window approach\n",
        "    and seasonal pattern incorporation\n",
        "\n",
        "    Args:\n",
        "        test_df: DataFrame containing test data\n",
        "        train_df: DataFrame containing training data\n",
        "        n_steps_in: Number of input time steps\n",
        "        n_steps_out: Number of output time steps\n",
        "        target_cols: List of target columns to predict\n",
        "        reset_window_days: How often to reset the prediction window with seasonal training data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with predictions for all test rows\n",
        "    \"\"\"\n",
        "    # Default target columns if none provided\n",
        "    if target_cols is None:\n",
        "        target_cols = ['Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction']\n",
        "\n",
        "    # Print information about the data\n",
        "    print(f\"Total rows in test data: {len(test_df)}\")\n",
        "    print(f\"Number of unique kingdoms: {test_df['kingdom'].nunique()}\")\n",
        "\n",
        "    # Preprocess training data to include cyclical features and wind direction encoding\n",
        "    train_df = preprocess_data(train_df)\n",
        "\n",
        "    # Keep track of whether Wind_Direction is among target columns\n",
        "    needs_wind_direction = 'Wind_Direction' in target_cols\n",
        "\n",
        "    # Train the model with enhanced features\n",
        "    results = run_pipeline(train_df, n_steps_in=n_steps_in, n_steps_out=n_steps_out, epochs=50)\n",
        "    model = results['model']\n",
        "    label_encoder = results['label_encoder']\n",
        "    scaler = results['scaler']\n",
        "    numerical_cols = results['numerical_cols']\n",
        "\n",
        "    # Find indices of wind direction components in numerical cols\n",
        "    wind_dir_sin_idx = -1\n",
        "    wind_dir_cos_idx = -1\n",
        "    if 'wind_dir_sin' in numerical_cols and 'wind_dir_cos' in numerical_cols:\n",
        "        wind_dir_sin_idx = numerical_cols.index('wind_dir_sin')\n",
        "        wind_dir_cos_idx = numerical_cols.index('wind_dir_cos')\n",
        "\n",
        "    # Preprocess test data including cyclical features and wind direction encoding\n",
        "    test_df = preprocess_data(test_df)\n",
        "\n",
        "    # Initialize results DataFrame with all test data\n",
        "    result_df = test_df[['ID']].copy()\n",
        "\n",
        "    # Make sure target columns exist in result_df\n",
        "    for col in target_cols:\n",
        "        if col not in result_df.columns:\n",
        "            result_df[col] = np.nan\n",
        "\n",
        "    # Track prediction coverage\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Group by kingdom to handle each location separately\n",
        "    # Sort first by kingdom, then by date to ensure chronological order\n",
        "    test_df = test_df.sort_values(by=['kingdom', 'Year', 'Month', 'Day'])\n",
        "    grouped = test_df.groupby('kingdom')\n",
        "\n",
        "    for kingdom, group in grouped:\n",
        "        print(f\"Processing kingdom: {kingdom}, rows: {len(group)}\")\n",
        "\n",
        "        # Get training data for this kingdom\n",
        "        train_kingdom = train_df[train_df['kingdom'] == kingdom].sort_values(by=['Year', 'Month', 'Day'])\n",
        "\n",
        "        # If kingdom not in training data, use average from all training data\n",
        "        if len(train_kingdom) == 0:\n",
        "            print(f\"Warning: No training data for {kingdom}. Using global averages.\")\n",
        "            train_kingdom = train_df.copy()\n",
        "\n",
        "        # Encode location\n",
        "        try:\n",
        "            location_encoded = label_encoder.transform([kingdom])[0]\n",
        "        except ValueError:\n",
        "            print(f\"Warning: {kingdom} not seen during training. Using a default kingdom.\")\n",
        "            # Use the first kingdom in the training data\n",
        "            default_kingdom = label_encoder.classes_[0]\n",
        "            location_encoded = label_encoder.transform([default_kingdom])[0]\n",
        "\n",
        "        # Track days since last window reset\n",
        "        days_since_reset = reset_window_days  # Start with a reset\n",
        "\n",
        "        # Create initial historical data from seasonally appropriate training data\n",
        "        first_row = group.iloc[0]\n",
        "        current_month = first_row['Month']\n",
        "        current_day = first_row['Day']\n",
        "\n",
        "        # Get seasonal data for initial window\n",
        "        seasonal_data = get_seasonal_matching_data(\n",
        "            train_df, current_month, current_day, kingdom, n_steps_in\n",
        "        )\n",
        "\n",
        "        if len(seasonal_data) >= n_steps_in:\n",
        "            # Use the most recent n_steps_in days from seasonal data\n",
        "            initial_history = seasonal_data.sort_values(\n",
        "                by=['Year', 'Month', 'Day']\n",
        "            ).tail(n_steps_in)[numerical_cols]\n",
        "        else:\n",
        "            # Use what we have and pad with means\n",
        "            initial_history = seasonal_data[numerical_cols]\n",
        "            # Calculate mean values for each feature\n",
        "            mean_values = train_df[numerical_cols].mean()\n",
        "            # Create padding with mean values\n",
        "            padding_rows = n_steps_in - len(initial_history)\n",
        "            padding = pd.DataFrame([mean_values] * padding_rows)\n",
        "            initial_history = pd.concat([padding, initial_history], ignore_index=True)\n",
        "\n",
        "        # Convert to numpy array\n",
        "        history = initial_history.values\n",
        "\n",
        "        # Store previous predictions for RMSE calculation and monitoring\n",
        "        previous_predictions = []\n",
        "        actual_values = []\n",
        "\n",
        "        # For each day in the test set for this kingdom, predict the values\n",
        "        for i in range(len(group)):\n",
        "            current_row = group.iloc[i]\n",
        "            current_id = current_row['ID']\n",
        "            current_month = current_row['Month']\n",
        "            current_day = current_row['Day']\n",
        "\n",
        "            # Check if we need to reset the window with fresh seasonal data\n",
        "            if days_since_reset >= reset_window_days:\n",
        "                print(f\"Resetting prediction window for {kingdom} at month {current_month}, day {current_day}\")\n",
        "\n",
        "                # Get seasonal matching data for current time\n",
        "                seasonal_data = get_seasonal_matching_data(\n",
        "                    train_df, current_month, current_day, kingdom, n_steps_in\n",
        "                )\n",
        "\n",
        "                if len(seasonal_data) >= n_steps_in:\n",
        "                    # Reset history with seasonal data\n",
        "                    seasonal_history = seasonal_data.sort_values(\n",
        "                        by=['Year', 'Month', 'Day']\n",
        "                    ).tail(n_steps_in)[numerical_cols].values\n",
        "\n",
        "                    # Blend current history with seasonal history (gradual transition)\n",
        "                    blend_factor = 0.7  # 70% seasonal, 30% current trajectory\n",
        "                    history = blend_factor * seasonal_history + (1 - blend_factor) * history\n",
        "\n",
        "                    # Reset counter\n",
        "                    days_since_reset = 0\n",
        "\n",
        "            # Create location input (same location for all time steps)\n",
        "            location_seq = np.full((1, n_steps_in, 1), location_encoded)\n",
        "\n",
        "            # Normalize features\n",
        "            history_df = pd.DataFrame(history, columns=numerical_cols)\n",
        "            feature_input_normalized = scaler.transform(history_df).reshape(1, n_steps_in, -1)\n",
        "\n",
        "            # Make prediction\n",
        "            prediction = model.predict([feature_input_normalized, location_seq], verbose=0)\n",
        "\n",
        "            # We're only interested in the first prediction (next day)\n",
        "            next_day_pred = prediction[0, 0, :]\n",
        "\n",
        "            # Convert back to original scale\n",
        "            next_day_pred_df = pd.DataFrame(next_day_pred.reshape(1, -1), columns=numerical_cols)\n",
        "            next_day_pred_original = scaler.inverse_transform(next_day_pred_df)\n",
        "\n",
        "            # Store prediction for monitoring\n",
        "            previous_predictions.append(next_day_pred_original[0])\n",
        "\n",
        "            # Handle Wind_Direction prediction from sine and cosine components if needed\n",
        "            if needs_wind_direction and wind_dir_sin_idx >= 0 and wind_dir_cos_idx >= 0:\n",
        "                # Extract sine and cosine components\n",
        "                wind_dir_sin = next_day_pred_original[0, wind_dir_sin_idx]\n",
        "                wind_dir_cos = next_day_pred_original[0, wind_dir_cos_idx]\n",
        "\n",
        "                # Convert back to degrees\n",
        "                wind_dir_degrees = wind_direction_from_components(wind_dir_sin, wind_dir_cos)\n",
        "\n",
        "                # Add to result DataFrame\n",
        "                result_df.loc[result_df['ID'] == current_id, 'Wind_Direction'] = wind_dir_degrees\n",
        "\n",
        "            # Update result DataFrame with predictions for other columns\n",
        "            for j, col in enumerate(numerical_cols):\n",
        "                # Skip wind_dir_sin and wind_dir_cos as we handle them separately above\n",
        "                if col in ['wind_dir_sin', 'wind_dir_cos']:\n",
        "                    continue\n",
        "\n",
        "                if col in target_cols:\n",
        "                    result_df.loc[result_df['ID'] == current_id, col] = next_day_pred_original[0, j]\n",
        "\n",
        "            # Count this prediction\n",
        "            total_predictions += 1\n",
        "\n",
        "            # Prepare for next day prediction by updating history\n",
        "            # Use actual values from test set where available\n",
        "            actual_values_for_next = []\n",
        "            for col_idx, col in enumerate(numerical_cols):\n",
        "                if col in ['wind_dir_sin', 'wind_dir_cos'] and 'Wind_Direction' in current_row and not pd.isna(current_row['Wind_Direction']):\n",
        "                    # Convert actual Wind_Direction to sine and cosine\n",
        "                    actual_wind_dir = current_row['Wind_Direction']\n",
        "                    if col == 'wind_dir_sin':\n",
        "                        actual_values_for_next.append(np.sin(np.radians(actual_wind_dir)))\n",
        "                    else:  # col == 'wind_dir_cos'\n",
        "                        actual_values_for_next.append(np.cos(np.radians(actual_wind_dir)))\n",
        "\n",
        "                    # Store for monitoring\n",
        "                    if col == 'wind_dir_sin' and needs_wind_direction:\n",
        "                        # Calculate the corresponding predicted Wind_Direction value\n",
        "                        wind_dir_sin = next_day_pred_original[0, wind_dir_sin_idx]\n",
        "                        wind_dir_cos = next_day_pred_original[0, wind_dir_cos_idx]\n",
        "                        predicted_wind_dir = wind_direction_from_components(wind_dir_sin, wind_dir_cos)\n",
        "                        actual_values.append(('Wind_Direction', actual_wind_dir, predicted_wind_dir))\n",
        "\n",
        "                elif col in current_row and not pd.isna(current_row[col]):\n",
        "                    # Use actual value from test data\n",
        "                    actual_values_for_next.append(current_row[col])\n",
        "\n",
        "                    # Store for monitoring if this is a target column\n",
        "                    if col in target_cols:\n",
        "                        actual_values.append((col, current_row[col], next_day_pred_original[0, col_idx]))\n",
        "                elif col in target_cols:\n",
        "                    # Use our prediction\n",
        "                    actual_values_for_next.append(next_day_pred_original[0, col_idx])\n",
        "                else:\n",
        "                    # Use the mean value from training\n",
        "                    actual_values_for_next.append(train_df[col].mean())\n",
        "\n",
        "            # Roll the history window forward\n",
        "            history = np.vstack([history[1:], actual_values_for_next])\n",
        "\n",
        "            # Increment days since last reset\n",
        "            days_since_reset += 1\n",
        "\n",
        "        # After processing kingdom, calculate and print RMSE for monitoring\n",
        "        if actual_values:\n",
        "            # Convert to DataFrame for easier analysis\n",
        "            actual_df = pd.DataFrame(actual_values, columns=['feature', 'actual', 'predicted'])\n",
        "\n",
        "            # Calculate RMSE by feature\n",
        "            for feature in actual_df['feature'].unique():\n",
        "                feature_df = actual_df[actual_df['feature'] == feature]\n",
        "                if feature == 'Wind_Direction':\n",
        "                    # Use circular metric for wind direction\n",
        "                    mae = circular_mae(feature_df['actual'], feature_df['predicted'])\n",
        "                    print(f\"Kingdom {kingdom}, Feature {feature}: Circular MAE = {mae:.4f}\")\n",
        "                else:\n",
        "                    # Use standard RMSE for other features\n",
        "                    rmse = np.sqrt(mean_squared_error(feature_df['actual'], feature_df['predicted']))\n",
        "                    print(f\"Kingdom {kingdom}, Feature {feature}: RMSE = {rmse:.4f}\")\n",
        "\n",
        "    # Verify we have predictions for all rows\n",
        "    print(f\"Made predictions for {total_predictions} of {len(test_df)} test rows\")\n",
        "\n",
        "    # Check how many rows have NaN values in target columns\n",
        "    nan_count = result_df[target_cols].isna().any(axis=1).sum()\n",
        "    if nan_count > 0:\n",
        "        print(f\"Warning: {nan_count} rows have NaN values in target columns\")\n",
        "\n",
        "    # Keep only ID and target columns in the final output\n",
        "    output_cols = ['ID'] + [col for col in target_cols if col in result_df.columns]\n",
        "    result_df = result_df[output_cols]\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    result_df.to_csv(\"drive/MyDrive/DataCrunch/predictions.csv\", index=False)\n",
        "    print(f\"Predictions saved to predictions.csv with {len(result_df)} rows\")\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Function to convert sine/cosine components back to degrees\n",
        "def wind_direction_from_components(sin_val, cos_val):\n",
        "    \"\"\"Convert sine and cosine components back to degrees\"\"\"\n",
        "    degrees = np.degrees(np.arctan2(sin_val, cos_val))\n",
        "    # Ensure result is in the range [0, 360)\n",
        "    return (degrees + 360) % 360"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wctqdzJHvoy",
        "outputId": "c69f7178-1216-4c7b-a79e-49ee78bb6f6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Starting hyperparameter tuning process...\n",
            "Features used in model: ['latitude', 'longitude', 'Avg_Temperature', 'Avg_Feels_Like_Temperature', 'Temperature_Range', 'Feels_Like_Temperature_Range', 'Radiation', 'Rain_Amount', 'Rain_Duration', 'Wind_Speed', 'Wind_Direction', 'Evapotranspiration', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'wind_dir_sin', 'wind_dir_cos']\n",
            "Training samples: 67968\n",
            "Validation samples: 8496\n",
            "Test samples: 8496\n",
            "Starting hyperparameter tuning...\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "0.0013775         |0.0013775         |learning_rate\n",
            "64                |64                |lstm_units\n",
            "8                 |8                 |embedding_dim\n",
            "0.3               |0.3               |dropout_rate\n",
            "relu              |relu              |activation\n",
            "\n",
            "Epoch 1/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 48ms/step - loss: 0.0499 - val_loss: 0.0254 - learning_rate: 0.0014\n",
            "Epoch 2/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 47ms/step - loss: 0.0277 - val_loss: 0.0232 - learning_rate: 0.0014\n",
            "Epoch 3/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 48ms/step - loss: 0.0245 - val_loss: 0.0210 - learning_rate: 0.0014\n",
            "Epoch 4/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 47ms/step - loss: 0.0221 - val_loss: 0.0187 - learning_rate: 0.0014\n",
            "Epoch 5/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 48ms/step - loss: 0.0203 - val_loss: 0.0169 - learning_rate: 0.0014\n",
            "Epoch 6/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 47ms/step - loss: 0.0186 - val_loss: 0.0169 - learning_rate: 0.0014\n",
            "Epoch 7/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 48ms/step - loss: 0.0168 - val_loss: 0.0156 - learning_rate: 2.7549e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 48ms/step - loss: 0.0163 - val_loss: 0.0155 - learning_rate: 2.7549e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 47ms/step - loss: 0.0160 - val_loss: 0.0153 - learning_rate: 2.7549e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 47ms/step - loss: 0.0158 - val_loss: 0.0155 - learning_rate: 2.7549e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 48ms/step - loss: 0.0154 - val_loss: 0.0151 - learning_rate: 5.5098e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 47ms/step - loss: 0.0153 - val_loss: 0.0151 - learning_rate: 5.5098e-05\n",
            "Epoch 13/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 48ms/step - loss: 0.0153 - val_loss: 0.0151 - learning_rate: 1.1020e-05\n",
            "Epoch 14/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 47ms/step - loss: 0.0152 - val_loss: 0.0151 - learning_rate: 2.2039e-06\n",
            "Epoch 1/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 47ms/step - loss: 0.0491 - val_loss: 0.0253 - learning_rate: 0.0014\n",
            "Epoch 2/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 48ms/step - loss: 0.0279 - val_loss: 0.0225 - learning_rate: 0.0014\n",
            "Epoch 3/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 48ms/step - loss: 0.0244 - val_loss: 0.0198 - learning_rate: 0.0014\n",
            "Epoch 4/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 48ms/step - loss: 0.0218 - val_loss: 0.0183 - learning_rate: 0.0014\n",
            "Epoch 5/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 48ms/step - loss: 0.0197 - val_loss: 0.0171 - learning_rate: 0.0014\n",
            "Epoch 6/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 48ms/step - loss: 0.0183 - val_loss: 0.0163 - learning_rate: 0.0014\n",
            "Epoch 7/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 48ms/step - loss: 0.0170 - val_loss: 0.0147 - learning_rate: 0.0014\n",
            "Epoch 8/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 47ms/step - loss: 0.0160 - val_loss: 0.0149 - learning_rate: 0.0014\n",
            "Epoch 9/50\n",
            "\u001b[1m4246/4246\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 48ms/step - loss: 0.0149 - val_loss: 0.0151 - learning_rate: 2.7549e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m 795/4246\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:50\u001b[0m 49ms/step - loss: 0.0149"
          ]
        }
      ],
      "source": [
        "# 4. Update the main execution block to use hyperparameter tuning\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Load training data\n",
        "    train_df = pd.read_csv(\"drive/MyDrive/DataCrunch/train.csv\")\n",
        "\n",
        "    # Load test data\n",
        "    test_df = pd.read_csv(\"drive/MyDrive/DataCrunch/test.csv\")\n",
        "\n",
        "    # Preprocess training data\n",
        "    train_df = preprocess_data(train_df)\n",
        "\n",
        "    # Run hyperparameter tuning\n",
        "    print(\"Starting hyperparameter tuning process...\")\n",
        "    results = run_pipeline_with_tuning(\n",
        "        df=train_df,\n",
        "        n_steps_in=30,\n",
        "        n_steps_out=7,\n",
        "        max_trials=10,  # Try 10 different hyperparameter combinations\n",
        "        execution_per_trial=2  # Run each trial twice to get more stable results\n",
        "    )\n",
        "\n",
        "    # Get the best model\n",
        "    best_model = results['model']\n",
        "    best_hyperparameters = results['best_hyperparameters']\n",
        "\n",
        "    # Save the best model\n",
        "    best_model.save('drive/MyDrive/DataCrunch/best_model.h5')\n",
        "\n",
        "    # Save hyperparameters for reference\n",
        "    with open('drive/MyDrive/DataCrunch/best_hyperparameters.txt', 'w') as f:\n",
        "        for param, value in best_hyperparameters.items():\n",
        "            f.write(f\"{param}: {value}\\n\")\n",
        "\n",
        "    # Specify target columns to predict\n",
        "    target_columns = ['Avg_Temperature', 'Radiation', 'Rain_Amount', 'Wind_Speed', 'Wind_Direction']\n",
        "\n",
        "    # Make predictions on test data with the best model\n",
        "    predictions = predict_on_test(\n",
        "        test_df=test_df,\n",
        "        train_df=train_df,\n",
        "        n_steps_in=30,\n",
        "        n_steps_out=7,\n",
        "        target_cols=target_columns,\n",
        "        reset_window_days=5  # Reset prediction window every 5 days\n",
        "    )\n",
        "\n",
        "    print(\"Prediction process completed with hyperparameter-tuned model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}